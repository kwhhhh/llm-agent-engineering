{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf0d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8000/items/99\"\n",
    "data = {\n",
    "    \"name\": \"Tablet\",\n",
    "    \"price\": 399.0,\n",
    "    \"is_offer\": None\n",
    "}\n",
    "\n",
    "response = requests.put(url, json=data)\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response JSON:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8643f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5944893965836374"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "56206 / (2657 + 35682 + 56206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c55c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.620000000000001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20.37 - 6.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065d8228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.109999999999992"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "42.26 + 20.59 - 45.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f209d87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(16, dtype=torch.float16).reshape(2,2,4)\n",
    "y = torch.pow(x, 2)\n",
    "eps = 1e-6\n",
    "z = torch.rsqrt(torch.mean(y, dim=2))\n",
    "x / z.unsqueeze(2)\n",
    "weight = torch.zeros(4)\n",
    "\n",
    "# x * (torch.rsqrt(torch.mean(torch.pow(x,2), dim=2, keepdim=True)) + eps)\n",
    "x * (torch.rsqrt(x.pow(2).mean(-1, keepdim=True)) + eps) * weight.float()\n",
    "# x / (torch.rsqrt(torch.mean(y, dim=2)) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c8b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11053395301886396\n",
      "0.018218886714342464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(5256 / 47551)\n",
    "print(5256 / 47551 - 2889 / 31295)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50807929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\18850\\anaconda3\\envs\\mimillm\\Lib\\site-packages\\torch\\cuda\\__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce GTX 1070 with Max-Q Design which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.5) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "c:\\Users\\18850\\anaconda3\\envs\\mimillm\\Lib\\site-packages\\torch\\cuda\\__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "c:\\Users\\18850\\anaconda3\\envs\\mimillm\\Lib\\site-packages\\torch\\cuda\\__init__.py:326: UserWarning: \n",
      "NVIDIA GeForce GTX 1070 with Max-Q Design with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce GTX 1070 with Max-Q Design GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847, 151645,\n",
      "            198, 151644,    872,    198,   5501,  19131,   6133,    264,   2632,\n",
      "           2699, 151645,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_id = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please introduce yourself a little bit\"}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generated_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "print(model_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1063a2b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# max_new_tokens 控制了模型最多能生成多少个新的Token\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 将生成的 Token ID 截取掉输入部分\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 这样我们只解码模型新生成的部分\u001b[39;00m\n\u001b[32m      8\u001b[39m generated_ids = [\n\u001b[32m      9\u001b[39m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mzip\u001b[39m(model_inputs.input_ids, generated_ids)\n\u001b[32m     11\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18850\\anaconda3\\envs\\mimillm\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18850\\anaconda3\\envs\\mimillm\\Lib\\site-packages\\transformers\\generation\\utils.py:2431\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2428\u001b[39m batch_size = inputs_tensor.shape[\u001b[32m0\u001b[39m]\n\u001b[32m   2430\u001b[39m device = inputs_tensor.device\n\u001b[32m-> \u001b[39m\u001b[32m2431\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_has_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2433\u001b[39m \u001b[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001b[39;00m\n\u001b[32m   2434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder:\n\u001b[32m   2435\u001b[39m     \u001b[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[32m   2436\u001b[39m     \u001b[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18850\\anaconda3\\envs\\mimillm\\Lib\\site-packages\\transformers\\generation\\utils.py:2111\u001b[39m, in \u001b[36mGenerationMixin._prepare_special_tokens\u001b[39m\u001b[34m(self, generation_config, kwargs_has_attention_mask, device)\u001b[39m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m decoder_start_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2106\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2107\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2108\u001b[39m     )\n\u001b[32m   2109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2110\u001b[39m     eos_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2111\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[43misin_mps_friendly\u001b[49m\u001b[43m(\u001b[49m\u001b[43melements\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token_tensor\u001b[49m\u001b[43m)\u001b[49m.any()\n\u001b[32m   2112\u001b[39m ):\n\u001b[32m   2113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kwargs_has_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs_has_attention_mask:\n\u001b[32m   2114\u001b[39m         logger.warning_once(\n\u001b[32m   2115\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe attention mask is not set and cannot be inferred from input because pad token is same as \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2116\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33meos token. As a consequence, you may observe unexpected behavior. Please pass your input\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2117\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`attention_mask` to obtain reliable results.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2118\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18850\\anaconda3\\envs\\mimillm\\Lib\\site-packages\\transformers\\pytorch_utils.py:345\u001b[39m, in \u001b[36misin_mps_friendly\u001b[39m\u001b[34m(elements, test_elements)\u001b[39m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elements.tile(test_elements.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m).eq(test_elements.unsqueeze(\u001b[32m1\u001b[39m)).sum(dim=\u001b[32m0\u001b[39m).bool().squeeze()\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43melements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# max_new_tokens 控制了模型最多能生成多少个新的Token\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "# 将生成的 Token ID 截取掉输入部分\n",
    "# 这样我们只解码模型新生成的部分\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in\n",
    "    zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "# 解码生成的 Token ID\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\n模型的回答:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimillm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
