{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d159a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-tdStLQVkiu1YaAGdhcWdrV4HkSij0aWVS7Lt7ZYkhWWp60Ib\"\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb1b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model = 'gpt-3.5-turbo'):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    client = OpenAI(base_url=\"https://api.cphone.vip/v1\")\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9b4286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m an AI language model created by OpenAI. I can help answer questions, provide explanations, assist with writing, and much more. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(\"What are you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f808c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arr, I be fuming that me blender lid flew off and \n",
    "splattered me kitchen walls with smoothie! And to make \n",
    "matters worse, the warranty don't cover the cost of \n",
    "cleaning up me kitchen. I need yer help right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9591efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"Chinese in a calm and offended tone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9efdfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tanslate the text that is  delimited by triple backticks into a style\n",
      "that is Chinese in a calm and offended tone.\n",
      "text: ```\n",
      "Arr, I be fuming that me blender lid flew off and \n",
      "splattered me kitchen walls with smoothie! And to make \n",
      "matters worse, the warranty don't cover the cost of \n",
      "cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\" Tanslate the text that is  \\\n",
    "delimited by triple backticks into a style\n",
    "that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60aa3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f9fc6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'唉，我真是气炸了，搅拌机的盖子竟然飞了出去，把厨房的墙壁都溅满了果昔！更糟糕的是，保修竟然不包括清理厨房的费用。现在我真是需要你的帮助，朋友！'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "737d6e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gzjcm\\anaconda3\\envs\\vllm_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\gzjcm\\anaconda3\\envs\\vllm_env\\lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={'max_input_tokens': 16385, 'max_output_tokens': 4096, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': False, 'structured_output': False, 'image_url_inputs': False, 'pdf_inputs': False, 'pdf_tool_message': False, 'image_tool_message': False, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x00000199802DC940>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000199BE915900>, root_client=<openai.OpenAI object at 0x00000199802DF220>, root_async_client=<openai.AsyncOpenAI object at 0x00000199BE9141F0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.cphone.vip/v1')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(\n",
    "    api_key = \"sk-tdStLQVkiu1YaAGdhcWdrV4HkSij0aWVS7Lt7ZYkhWWp60Ib\",\n",
    "    base_url=\"https://api.cphone.vip/v1\",\n",
    "    temperature=0,\n",
    "    )\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b34706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\" Tanslate the text that is\n",
    "delimited by triple backticks into a style\n",
    "that is {style}.\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a52fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52e8deb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template=' Tanslate the text that is\\ndelimited by triple backticks into a style\\nthat is {style}.\\ntext: ```{text}```\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f45a89db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b615a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"Chinese in a calm and offended tone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b54bd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arr, I be fuming that me blender lid flew off and \n",
    "splattered me kitchen walls with smoothie! And to make \n",
    "matters worse, the warranty don't cover the cost of \n",
    "cleaning up me kitchen. I need yer help right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30668769",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "    style = customer_style,\n",
    "    text = customer_email\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4255bfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04cf6542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" Tanslate the text that is\\ndelimited by triple backticks into a style\\nthat is Chinese in a calm and offended tone.\\ntext: ```\\nArr, I be fuming that me blender lid flew off and \\nsplattered me kitchen walls with smoothie! And to make \\nmatters worse, the warranty don't cover the cost of \\ncleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e639628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_response = chat.invoke(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6323a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唉，我真是气炸了，搅拌机的盖子竟然飞了出去，把厨房的墙壁全都溅满了果昔！更糟糕的是，保修竟然不包括清理厨房的费用。真是让人难以接受，朋友，我现在真的需要你的帮助。\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37cfa15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"They key customer,\n",
    "the warranty does not cover\n",
    "cleaning up the kitchen\n",
    "because it's your fault that\n",
    "you missed your blender\n",
    "and forgetting to put the lid on before\n",
    "starting the blender.\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7d9c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"a polite tone that speaks in English Pirate\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2791aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style = service_style_pirate,\n",
    "    text = service_reply\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "459f6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_response = chat.invoke(service_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46cbd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy, esteemed customer,  \n",
      "Pray understand, the warranty be not coverin’ the tidyin’ of the galley,  \n",
      "for ’tis yer own fault ye missed yer blender,  \n",
      "and forgot to secure the lid afore ye set sail with the blender.  \n",
      "Aye, tough luck it be! Fair winds to ye!\n"
     ]
    }
   ],
   "source": [
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6ac0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"This leaf blower is pretty amazing. It has four settings:\\ candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our law \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\"\"\"\n",
    "\n",
    "review_template = \"\"\"For the following text, extract the following information:\n",
    "gift: Was the item purchased as a gift for someone else? Ans\n",
    "delivery_days: How many days did it take for the product to \n",
    "price_value: Extract any sentences about the value or price,\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc49eccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\ngift: Was the item purchased as a gift for someone else? Ans\\ndelivery_days: How many days did it take for the product to \\nprice_value: Extract any sentences about the value or price,\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b007381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"gift\": \"Yes, it was purchased as an anniversary present for the user's wife.\",\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text = customer_review)\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aeab82c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4baaf12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.prompts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResponseSchema\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.prompts'"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "ChatPromptTemplate.from_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e38542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
      "中山陵,夫子庙,玄武湖\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "initial_value must be str or None, not AIMessage",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m response \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39minvoke(final_prompt)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m---> 23\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(ret)\n",
      "File \u001b[1;32mc:\\Users\\gzjcm\\anaconda3\\envs\\vllm_env\\lib\\site-packages\\langchain_core\\output_parsers\\list.py:176\u001b[0m, in \u001b[0;36mCommaSeparatedListOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    A list of strings.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(\n\u001b[1;32m--> 176\u001b[0m         \u001b[43mStringIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m, quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, skipinitialspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m reader \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m csv\u001b[38;5;241m.\u001b[39mError:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# Keep old logic for backup\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: initial_value must be str or None, not AIMessage"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{parser_instructions}\"),\n",
    "    (\"human\", \"列出{cityName}的{viewPointNum}个著名景点。\")\n",
    "])\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "parser_instructions = output_parser.get_format_instructions()\n",
    "# 查看解析器的指令内容\n",
    "print(parser_instructions)\n",
    "\n",
    "# final_prompt = prompt.invoke({\"cityName\": \"南京\", \"viewPointNum\": 3, \"parser_instructions\": parser_instructions})\n",
    "final_prompt = prompt.format_messages(cityName=\"南京\",\n",
    "                                      viewPointNum=3,\n",
    "                                      parser_instructions=parser_instructions)\n",
    "\n",
    "response = chat.invoke(final_prompt)\n",
    "print(response.content)\n",
    "\n",
    "ret = output_parser.parse(response)\n",
    "print(ret)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
